---
layout: post
title: A Minimalist End-to-End Scrapy Tutorial
tags: tutorial open-source
---

This tutorial series was first published on [Medium](https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0). I combine 5 parts into this long post for people who cannot access Medium.

Web scraping is an important skill for data scientists. I have developed a number of ad hoc web scraping projects using Python, BeautifulSoup, and Scrapy in the past few years and read a few books and tons of online tutorials along the way. However, I have not found a simple beginner level tutorial that is end-to-end in the sense that covers all basic steps and concepts in a typical Scrapy web scraping project (therefore Minimalist in the title) - that's why I am writing this and hope the code repo can serve as a template to help jumpstart your web scraping projects.


Many people ask: should I use BeautifulSoup or Scrapy? They are different things: BeautifulSoup is a library for parsing HTML and XML and Scrapy is a web scraping framework. You can use BeautifulSoup instead of Scrapy build-in selectors if you want but comparing BeautifulSoup to Scrapy is like comparing the Mac keyboard to the iMac or a better metaphor as stated in the official documentation "like comparing jinja2 to Django" if you know what they are :) - In short, you should learn Scrapy if you want to do serious and systematic web scraping.

TL;DR, show me the code: 
- [scrapy tutorial](https://github.com/harrywang/scrapy-tutorial) 
- [scrapy + selenium tutorial](https://github.com/harrywang/scrapy-selenium-demo)

In this tutorial series, I am going to cover the following steps:

- Start a Scrapy project from scratch and develop a simple spider. One important thing is the use of Scrapy Shell for analyzing pages and debugging, which is one of the main reasons you should use Scrapy over BeautifulSoup.
- Introduce Item and ItemLoader and explain why you want to use them (although they make your code seem more complicated at first).
- Store the data to the database using ORM (SQLAlchemy) via Pipelines and show how to set up the most common One-to-Many and Many-to-Many relationships.
- Deploy the project to Scrapinghub (you have to pay for service such as scheduled crawling jobs) or set up your own servers completely free of charge by using the great open source project ScrapydWeb and Heroku.
- I created a separate repo (Scrapy + Selenium) to show how to crawl dynamic web pages (such as a page that loads additional content via scrolling) and how to use proxy networks (ProxyMesh) to avoid getting banned.

Some prerequisites:
- Basic knowledge on Python (Python 3 for this tutorial), virtual environment, Homebrew, etc., see my other article for how to set up the environment: How to Setup Mac for Python Development
- Basic knowledge of Git and Github. I recommend the Pro Git book.
- Basic knowledge of database and ORM, e.g., Introduction to Structured Query Language (SQL).

Let's get started!
First, create a new folder, setup Python 3 virtual environment inside the folder, and install Scrapy. To make this step easy, I created a starter repo, which you can fork and clone (see Python3 virtual environment documentation if needed):

```
$ git clone https://github.com/yourusername/scrapy-tutorial-starter.git
$ cd scrapy-tutorial-starter
$ python3.6 -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt
```